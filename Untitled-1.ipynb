import pandas as pd
import tensorflow as tf
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score

# Define the CSV links
train_data_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/train.csv"
test_data_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/test.csv"
gender_submission_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/gender_submission.csv"

# Read the train data
train_data = pd.read_csv(train_data_url)

# Read the test data
test_data = pd.read_csv(test_data_url)

# Read the gender submission data
gender_submission_data = pd.read_csv(gender_submission_url)

# Print confirmation messages
print(f"Train data loaded successfully! Shape: {train_data.shape}")
print(f"Test data loaded successfully! Shape: {test_data.shape}")
print(f"Gender submission data loaded successfully! Shape: {gender_submission_data.shape}")

# Check for missing values and handle them
train_data.isnull().sum()
# Handle missing values here

# Identify and handle categorical features
categorical_features = list(train_data.select_dtypes(include=['object']).columns)
for feature in categorical_features:
    # One-hot encode categorical features
    train_data = pd.concat([train_data, pd.get_dummies(train_data[feature], drop_first=True)], axis=1)
    test_data = pd.concat([test_data, pd.get_dummies(test_data[feature], drop_first=True)], axis=1)

# Separate features and target variable
features = train_data.drop(["Survived"], axis=1)
target = train_data["Survived"]

# Neural Network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(features.shape[1],)),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(features, target, epochs=10)

# Logistic Regression with GridSearchCV
param_grid = {"C": [0.1, 1, 10, 100], "penalty": ["l1", "l2"]}
grid_search_lr = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search_lr.fit(features, target)
best_model_lr = grid_search_lr.best_estimator_

# Random Forest with GridSearchCV
param_grid = {"n_estimators": [100, 200, 500], "max_depth": [3, 5, 7]}
grid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search_rf.fit(features, target)
best_model_rf = grid_search_rf.best_estimator_

# Evaluate Neural Network
y_pred_nn = model.predict(test_data)
f1_score_nn = f1_score(gender_submission_data["Survived"], y_pred_nn.round())

# Evaluate Logistic Regression
y_pred_lr = best_model_lr.predict(test_data)
f1_score_lr = f1_score(gender_submission_data["Survived"], y_pred_lr)

# Evaluate Random Forest
y_pred_rf = best_model_rf.predict(test_data)
f1_score_rf = f1_score(gender_submission_data["Survived"], y_pred_rf)

# Print F1 scores for all models
print("F1 Scores:")
print("Neural Network:", f1_score_nn)
print("Logistic Regression:", f1_score_lr)
print("Random Forest:", f1_score_rf)
