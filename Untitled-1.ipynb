import pandas as pd

# Define the CSV links
train_data_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/train.csv"
test_data_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/test.csv"
gender_submission_url = "https://raw.githubusercontent.com/Ibrahimmohamed111/Titanic-machine-learning-dataset/main/gender_submission.csv"

# Read the train data
train_data = pd.read_csv(train_data_url)

# Read the test data
test_data = pd.read_csv(test_data_url)

# Read the gender submission data
gender_submission_data = pd.read_csv(gender_submission_url)

# Print confirmation messages
print(f"Train data loaded successfully! Shape: {train_data.shape}")
print(f"Test data loaded successfully! Shape: {test_data.shape}")
print(f"Gender submission data loaded successfully! Shape: {gender_submission_data.shape}")

# Check for missing values and handle them
train_data.isnull().sum()
# Impute missing values or drop rows with many missing values

# Identify and handle categorical features
categorical_features = list(train_data.filter(like='object'))
for feature in categorical_features:
    # One-hot encode categorical features
    train_data[feature] = pd.get_dummies(train_data[feature], drop_first=True)
    test_data[feature] = pd.get_dummies(test_data[feature], drop_first=True)

# Separate features and target variable
features = train_data.drop("Survived", axis=1)
target = train_data["Survived"]

# Neural Network
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation="relu", input_shape=(features.shape[1],)),
    tf.keras.layers.Dense(64, activation="relu"),
    tf.keras.layers.Dense(32, activation="relu"),
    tf.keras.layers.Dense(1, activation="sigmoid")
])
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
model.fit(features, target, epochs=10)

# Logistic Regression with GridSearchCV
param_grid = {"C": [0.1, 1, 10, 100], "penalty": ["l1", "l2"]}
grid_search = GridSearchCV(LogisticRegression(), param_grid, cv=5)
grid_search.fit(features, target)
best_model_lr = grid_search.best_estimator_

# Random Forest with GridSearchCV
param_grid = {"n_estimators": [100, 200, 500], "max_depth": [3, 5, 7]}
grid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)
grid_search.fit(features, target)
best_model_rf = grid_search.best_estimator_

# Evaluate Neural Network
y_pred_nn = model.predict(test_data)
f1_score_nn = f1_score(test_data["Survived"], y_pred_nn)

# Evaluate Logistic Regression
y_pred_lr = best_model_lr.predict(test_data)
f1_score_lr = f1_score(test_data["Survived"], y_pred_lr)

# Evaluate Random Forest
y_pred_rf = best_model_rf.predict(test_data)
f1_score_rf = f1_score(test_data["Survived"], y_pred_rf)

# Print F1 scores for all models
print("F1 Scores:")
print("Neural Network:", f1_score_nn)
print("Logistic Regression:", f1_score_lr)
print("Random Forest:", f1_score_rf)

